{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = torch.load(\"/home/drc/Downloads/cifar10-d875770b.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =torch.utils.model_zoo.load_url('https://download.pytorch.org/models/vgg11-bbd30ac9.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'features.0.weight',\n",
       " u'features.0.bias',\n",
       " u'features.3.weight',\n",
       " u'features.3.bias',\n",
       " u'features.6.weight',\n",
       " u'features.6.bias',\n",
       " u'features.8.weight',\n",
       " u'features.8.bias',\n",
       " u'features.11.weight',\n",
       " u'features.11.bias',\n",
       " u'features.13.weight',\n",
       " u'features.13.bias',\n",
       " u'features.16.weight',\n",
       " u'features.16.bias',\n",
       " u'features.18.weight',\n",
       " u'features.18.bias',\n",
       " u'classifier.0.weight',\n",
       " u'classifier.0.bias',\n",
       " u'classifier.3.weight',\n",
       " u'classifier.3.bias',\n",
       " u'classifier.6.weight',\n",
       " u'classifier.6.bias']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attempt to prune filters using taylor criterion\n",
    "\n",
    "\n",
    "class PruneWrap():\n",
    "    def __init__(self, model,gpu=False):\n",
    "        #model params\n",
    "        self.model = model\n",
    "        self.gpu = gpu\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        #bookkeeping\n",
    "        self.activations = []\n",
    "        self.gradients = []\n",
    "        self.grad_index = 0\n",
    "        self.activation_to_layer = {}\n",
    "        self.filter_ranks = {}\n",
    "        #callbacks for gradient updates\n",
    "        register_callbacks()\n",
    "    def register_callbacks(self):\n",
    "        activation_index = 0\n",
    "        for layer, (name, module) in enumerate(self.model.features._modules.items()):\n",
    "            x = module(x)\n",
    "            if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "                x.register_hook(self.compute_rank)\n",
    "                self.activations.append(x)\n",
    "                self.activation_to_layer[activation_index] = layer\n",
    "                activation_index += 1\n",
    "    def test(self,test_data_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (batch, label) in enumerate(test_data_loader):\n",
    "            batch = batch.cuda() if gpu else batch\n",
    "            output = model(Variable(batch))\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.cpu().eq(label).sum()\n",
    "            total += label.size(0)\n",
    "        print \"acc :\", float(correct) / total\n",
    "    \n",
    "    def train(self, optimizer = None, epoches = 10, prune=False):\n",
    "        if optimizer is None:\n",
    "            optimizer = optim.SGD(model.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "        for i in range(epoches):\n",
    "            print \"epoch: \", i\n",
    "            for batch, label in self.train_data_loader:\n",
    "                batch,label = (batch.cuda(),label.cuda) if self.gpu\n",
    "                self.model.zero_grad()\n",
    "                input = Variable(batch)\n",
    "                if prune:\n",
    "                    output = self.prunner.forward(input)\n",
    "                    self.criterion(output, Variable(label)).backward()\n",
    "                else:\n",
    "                    self.criterion(self.model(input), Variable(label)).backward()\n",
    "                    optimizer.step()\n",
    "        print \"Finished fine tuning.\"\n",
    "    def rank_prunes(self):\n",
    "        for batch, label in self.train_data_loader:\n",
    "                batch,label = (batch.cuda(),label.cuda) if self.gpu\n",
    "                self.model.zero_grad()\n",
    "                input = Variable(batch)\n",
    "        \n",
    "        self.prunner.normalize_ranks_per_layer()\n",
    "\n",
    "        return self.prunner.get_prunning_plan(num_filters_to_prune)\n",
    "    def prune():\n",
    "        self.test()\n",
    "    def normalize_ranks_per_layer(self):\n",
    "        for i in self.filter_ranks:\n",
    "            v = torch.abs(self.filter_ranks[i])\n",
    "            v = v / np.sqrt(torch.sum(v * v))\n",
    "    def compute_rank(self, grad):\n",
    "        activation_index = len(self.activations) - self.grad_index - 1\n",
    "        activation = self.activations[activation_index]\n",
    "        values = torch.sum((activation * grad), dim = 0).sum(dim=2).sum(dim=3)[0, :, 0, 0].data\n",
    "\n",
    "        # Normalize the rank by the filter dimensions\n",
    "        values = values / (activation.size(0) * activation.size(2) * activation.size(3))\n",
    "\n",
    "        if activation_index not in self.filter_ranks:\n",
    "            self.filter_ranks[activation_index] = torch.FloatTensor(activation.size(1)).zero_().cuda()\n",
    "\n",
    "        self.filter_ranks[activation_index] += values\n",
    "        self.grad_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attempt to prune for sparsity without wrapper thing\n",
    "class PruneWrapSparse():\n",
    "    def __init__(self,epoch_start,model,sparsity_initial,sparsity_target,prune_steps,update_rate,train,gpu=False):\n",
    "        #model params\n",
    "        self.model = model.float()\n",
    "        self.gpu = gpu\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.train_loader = train\n",
    "        self.layers_to_prune = [0,0,0]\n",
    "        \n",
    "        #bookkeeping\n",
    "        self.current_sparsity=0\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_initial = sparsity_initial\n",
    "        self.train_step = epoch_start\n",
    "        self.prune_steps =prune_steps\n",
    "        self.update_rate = update_rate\n",
    "        self.prunning = True\n",
    "        self.debug=.90\n",
    "        \n",
    "    def to_string(self):\n",
    "        print \"current sparsity: \" + str(self.current_sparsity)\n",
    "        print \"sparsity target: \"+ str(self.sparsity_target)\n",
    "        print \"initial sparsity: \" + str(self.sparsity_initial)\n",
    "        print \"current train step: \"+ str(self.train_step)\n",
    "        print \"prune steps: \"+str(self.prune_steps)\n",
    "        print \"prunning rate: \"+str(self.update_rate)\n",
    "        \n",
    "    def test(self,test_data_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (batch, label) in enumerate(test_data_loader):\n",
    "            batch,label = (batch.cuda(),label.cuda()) if self.gpu else (batch,label)\n",
    "            output = self.model(Variable(batch.float()))\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.cpu().eq(label).sum()\n",
    "            total += label.size(0)\n",
    "        print \"acc :\", float(correct) / total\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_sparsity=0\n",
    "        self.prune_step =0\n",
    "    \n",
    "    def compute_current_target(self):\n",
    "        return self.sparsity_target+(self.sparsity_initial-self.sparsity_target)*(1-self.train_step/(self.prune_steps*self.train_step))**3\n",
    "    \n",
    "    def train(self, optimizer = None, epoches = 10):\n",
    "        start = time.time()\n",
    "        if optimizer is None:\n",
    "            optimizer = optim.SGD(self.model.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "        for i in range(epoches):\n",
    "            print \"epoch: \", i\n",
    "            for batch, label in self.train_loader:\n",
    "                if self.current_sparsity >= self.sparsity_target:\n",
    "                    self.prunning = False\n",
    "                batch,label = (batch.cuda(),label.cuda()) if self.gpu else (batch,label)\n",
    "                input = Variable(batch)\n",
    "                if self.prunning and self.train_step%self.update_rate==0:\n",
    "                    print \"prunning\"\n",
    "                    self.prune()\n",
    "                    return\n",
    "                self.criterion(self.model(input), Variable(label)).backward()\n",
    "                optimizer.step()\n",
    "                self.train_step+=1\n",
    "        print \"Finished fine tuning.\"\n",
    "        print \"Time elapsed:\" + (time.time() - start)\n",
    "    \n",
    "    def prune(self):#all conv layers for now\n",
    "        current_sparsity_target = self.compute_current_target()\n",
    "        current_sparsity_target = self.debug  \n",
    "        for layer in list(self.model.children())[0]:\n",
    "            if isinstance(layer, nn.modules.conv.Conv2d) or isinstance(layer, nn.modules.Linear):\n",
    "                #find weight threshold for layer\n",
    "                weight_threshold = self.get_weight_threshold(layer.parameters(),current_sparsity_target) \n",
    "                print(list(layer.parameters()))\n",
    "                #prune layer\n",
    "                print()\n",
    "                self.save_weight_hist(list(layer.parameters())[0])\n",
    "                print weight_threshold,\"here\" \n",
    "                self.prune_layer(layer, weight_threshold)\n",
    "                self.save_weight_hist(list(layer.parameters())[0])\n",
    "            break\n",
    "        self.current_sparsity = current_sparsity_target\n",
    "        self.debug+=.02\n",
    "        \n",
    "    def prune_layer(self,layer,threshold):\n",
    "        dim = layer.weight.data.size()\n",
    "        for i in range(dim[0]):\n",
    "            for j in range(dim[1]):\n",
    "                for k in range(dim[2]):\n",
    "                    for l in range(dim[3]):\n",
    "                        #if layer.weight.data[i,j,k,l]<threshold:\n",
    "                            layer.weight.data[i,j,k,l]=0\n",
    "                            layer.weight[i,j,k,l].detach()\n",
    "    def save_weight_hist(self,params):\n",
    "        plt.figure()\n",
    "        plt.hist(list(params.cpu().data.abs().numpy().flatten()))\n",
    "        plt.show()\n",
    "    def get_weight_threshold(self,params,percent_to_prune):\n",
    "        weights=[]\n",
    "        for param in params:\n",
    "            if len(list(param.cpu().data.abs().numpy().flatten()))>1:\n",
    "                weights.extend(list(param.cpu().data.abs().numpy().flatten()))\n",
    "        threshold = np.percentile(np.array(weights),percent_to_prune)\n",
    "        print len(np.nonzero(np.array(weights)))/float(len(weights))\n",
    "        return threshold\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
